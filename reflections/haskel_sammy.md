Paper 1

Full Citation:
Killion, V. L. (2025, May 20). The TAKE IT DOWN Act: A Federal Law Prohibiting the Nonconsensual Publication of Intimate Images (LSB11314 Version 2). Congressional Research Service.

Link:
https://www.congress.gov/crs-product/LSB11314

Summary:
The TAKE IT DOWN Act, signed into law on May 19, 2025, criminalizes the nonconsensual publication of intimate images and "digital forgeries" (deepfakes). It amends the Communications Act of 1934 to establish seven new criminal offenses, including publication and threats involving depictions of both adults and minors. Additionally, it mandates that "covered platforms" implement a notice-and-removal process to take down such content within 48 hours of a victim's request.

Insights:
Dual Enforcement: The law utilizes criminal prohibitions enforced by the DOJ and civil regulatory requirements (notice-and-removal) enforced by the FTC as "unfair or deceptive acts or practices".

Broad "Digital Forgery" Scope: Deepfakes created via AI or other technological means are explicitly covered, addressing a gap in previous federal civil actions.

Strict Timelines: Platforms must remove reported depictions "as soon as possible" and no later than 48 hours after notice.

Limitations/risks:
Ambiguous Terms: Key terms like "publish" are not defined, potentially leading to legal disputes over whether private direct messages (DMs) are covered.

First Amendment Scrutiny: As a content-based regulation of speech, the Act may face "strict scrutiny" in court, requiring the government to prove it is the least restrictive means to achieve its goal.

Section 230 Conflicts: It remains unclear if the Act implicitly repeals Section 230 immunity for platforms regarding FTC enforcement actions.

Idea:
To help small platforms comply, the FTC could provide a standardized, "plain language" notice-and-removal template to ensure consistency and legal safety for both victims and providers.

Paper 2

Full Citation:
Pazzanese, C. (2026, January 28). How AI deepfakes have skirted revenge porn laws. Harvard Gazette.

Link:
https://news.harvard.edu/gazette/story/2026/01/how-ai-deepfakes-have-skirted-revenge-porn-laws/

Summary:
This article explores the legal challenges posed by AI-generated deepfakes, which often bypass traditional "revenge porn" laws because the images are not "authentic" photographs. Harvard Law Professor Rebecca Tushnet discusses the difficulty of regulating these tools in the U.S. due to robust First Amendment protections and the unprecedented scale and realism AI provides.

Insights:
Scale vs. Kind: The extreme realism and ease of AI generation may represent a shift where a difference in degree becomes a "difference in kind," intensifying the injury to victims.

The "Insurer" Liability Model: U.S. legal culture often treats defendants as "insurers"—liable for any failure to follow the law—whereas European regulators are more likely to forgive entities making "good-faith efforts".

Open-Source Accessibility: Even if major AI models (like Grok) implement guardrails, individuals can still train "well-behaved" models on explicit data, making complete prevention nearly impossible.

Limitations/risks:
Lack of Guardrail Mandates: Currently, there is no federal law requiring AI toolmakers to implement specific safety guardrails during development.

Political Speech Overlap: Critics argue that banning sexualized imagery could inadvertently suppress "sexualized mockery" used in political speech, such as historical satirical pornography.

Tool vs. User: There is no legal consensus on whether liability should fall solely on the user (like gun laws) or if the toolmaker should be automatically liable for misuse.

Idea:
To address the "scrambling" nature of current regulations , the U.S. could develop a Hybrid Liability Framework that bridges the gap between the "user-only" responsibility and the "toolmaker-as-insurer" model .
